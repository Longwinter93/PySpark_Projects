# PySpark Projects
<br /> ***Apache Spark*** is a unified analytics engine for large-scale data processing. 
<br />It offers high-level APIs in Scala, Java, Python and R. It has an optimized engine that supports general computation graphs for data analysis.
<br />What is more, ***Apache Spark*** supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLib for machine learning, GraphX for graph processing and Structured Streaming for stream processing.
<br />**Spark** application consists of a driver program that runs the user's main function and executes various parallel operations on a cluster. 
<br />The main abstraction Spark provides is a resilient distributed dataset **(RDD)**, which is a collection of immutable elements partitioned across the nodes of the cluster that can be operated on in parallel. 
<br />It is also possible to ask Spark to persist an **RDD** in memory, allowing it to be reused efficiently across parallel operations. Last but not least, **RDDs** automatically recover from node failures.

<br />The second abstraction in Spark is **shared variables** that can be used in parallel operations. Spark supports two types of shared variables: **broadcast** variables and **accumulators** variables.

<br />This repository is made up of five projects.
<br />All descriptions are related to these projects are included in each folder.

## PySpark Projects:
- ### [PySpark_Project1_Cars](https://github.com/Longwinter93/PySpark_Projects/tree/main/PySpark_Project1_Cars)
- ### [PySpark_Project2_Lego](https://github.com/Longwinter93/PySpark_Projects/tree/main/PySpark_Project2_Lego)
- ### [PySpark_Project3_NewYorkCityPropertySales](https://github.com/Longwinter93/PySpark_Projects/tree/main/PySpark_Project3_NewYorkCityPropertySales)
- ### [PySpark_Project4_PizzaRestaurant](https://github.com/Longwinter93/PySpark_Projects/tree/main/PySpark_Project4_PizzaRestaurant)
- ### [PySpark_Project5_RDD](https://github.com/Longwinter93/PySpark_Projects/tree/main/PySpark_Project5_RDD)

###### To create README, this site [Apache Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html#overview) was used.
