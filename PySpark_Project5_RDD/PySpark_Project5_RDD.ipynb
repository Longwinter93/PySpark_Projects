{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,functions, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.pandas as ps\n",
    "from pyspark import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Spark Session and Creating a SparkSession called Resilient Distributed Dataset RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Resilient Distributed Dataset (RDD) Training\") \\\n",
    "        .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "## running Spark locally with 4 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://POL-8625NG3.SIIPOLSKA.PL:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Resilient Distributed Dataset (RDD) Training</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c74eb806a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001C74EB806A0>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,1,2,2,3,3,4,4,5,6,7,8,9,10,11,12,13,14,15,25,99,124]\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Actions` in PySpark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 3, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 25, 99, 124]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 2, 2, 3], [3, 4, 4, 5, 6], [7, 8, 9, 10, 11], [12, 13, 14, 15, 25, 99, 124]]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 12, 124, 1, 5, 9, 13, 25, 2, 6, 10, 14, 3, 7, 11, 15, 99]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "first_rdd = rdd\n",
    "print(first_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "take_rdd = rdd\n",
    "print(take_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n"
     ]
    }
   ],
   "source": [
    "reduce_rdd = rdd \n",
    "print(reduce_rdd.reduce(lambda x, y : x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 1), (2, 1), (2, 1), (3, 1), (3, 1), (4, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (25, 1), (99, 1), (124, 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: (x,1))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding maximum element by `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda x,y: x if x > y else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.fold(0, lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum:  124\n",
      "Minimum:  1\n",
      "Mean (average):  17.181818181818183\n",
      "Standard deviation:  30.583174761826097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(count: 22, mean: 17.181818181818183, stdev: 30.583174761826097, max: 124.0, min: 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Maximum: \", rdd.max())\n",
    "print(\"Minimum: \", rdd.min())\n",
    "print(\"Mean (average): \", rdd.mean())\n",
    "print(\"Standard deviation: \", rdd.stdev())\n",
    "rdd.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the longest word using `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horizons'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'My name is Lukasz. I am learning PySpark and RDD PySpark to broaden my horizons in Big Data'.split(' ')\n",
    "wordRDD=spark.sparkContext.parallelize(words)\n",
    "wordRDD.reduce(lambda w,v: w if len(w)>len(v) else v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can write regular Python functions to use with `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largerThan(x,y):\n",
    "    if len(x)> len(y):\n",
    "        return x\n",
    "    elif len(y) > len(x):\n",
    "        return y\n",
    "    else:\n",
    "        if x < y: return x\n",
    "        else: return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horizons'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sampling` an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample1= [3, 4, 9, 10, 13]\n",
      "Sample2= [1, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# get a sample whose expected size is m\n",
    "#note that the size of the sample is different in different runs\n",
    "m=5\n",
    "n=20\n",
    "print('Sample1=', rdd.sample(False,m/n).collect())\n",
    "print('Sample2=', rdd.sample(False,m/n).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving `RDD` as a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rdd = rdd\n",
    "save_rdd.saveAsTextFile('fileActions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Transformations` in PySpark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "print(rdd.map(lambda x: x +10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Map` operation with regular Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_if_odd(x):\n",
    "    if x%2==1:\n",
    "        return x*x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 9, 4, 25, 6, 49]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(square_if_odd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.filter(lambda x: x%2 ==0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25), (6, 36), (7, 49)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x:(x, x*x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `filter` to return RDD with elements divisible by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x:x%3==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lukasz', 'Lucjan', 'Ligrecja', 'Lucja']\n"
     ]
    }
   ],
   "source": [
    "data = ['Lukasz','Lucjan','Ligrecja','Lucja','Adam','Marcin','Rafal','Artur']\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.filter(lambda x: x.startswith(\"L\")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "data = [2,4,5,6,7,8,9]\n",
    "union_inp = spark.sparkContext.parallelize(data)\n",
    "union_rdd_1 = union_inp.filter(lambda x: x % 2 == 0)\n",
    "union_rdd_2 = union_inp.filter(lambda x: x % 3 ==0)\n",
    "print(union_rdd_1.union(union_rdd_2).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `flatmap` method returns a new RDD by first applying a function to all elements of this RDD, then flattening the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.flatMap(lambda x:(x, x*x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'All!',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Lukasz',\n",
       " 'I',\n",
       " 'learn',\n",
       " 'PySpark',\n",
       " 'RDD',\n",
       " 'Transformations']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Hey All!', \"My name is Lukasz\", 'I learn PySpark RDD Transformations']\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "(rdd.flatMap(lambda x: x.split(\" \")).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark `Pair RDD Operations & Transformations` in Pair RDDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lukasz', 90), ('Maciej', 87), ('Artur', 64), ('Dawid', 25), ('Bartosz', 76)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [('Lukasz', 90), ('Maciej', 87), ('Artur', 64), ('Dawid', 25), ('Bartosz', 76)]\n",
    "rdd = spark.sparkContext.parallelize(scores)\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lukasz', 181),\n",
       " ('Maciej', 169),\n",
       " ('Artur', 127),\n",
       " ('Dawid', 49),\n",
       " ('Bartosz', 153)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [('Lukasz', 90),\n",
    "           ('Maciej', 87), \n",
    "           ('Artur', 64),\n",
    "             ('Dawid', 25), \n",
    "             ('Bartosz', 76),\n",
    "             ('Lukasz', 91),\n",
    "           ('Maciej', 82), \n",
    "           ('Artur', 63),\n",
    "             ('Dawid', 24), \n",
    "             ('Bartosz', 77)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(scores)\n",
    "rdd.reduceByKey(lambda x, y: x+ y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artur', 64),\n",
       " ('Artur', 63),\n",
       " ('Bartosz', 76),\n",
       " ('Bartosz', 77),\n",
       " ('Dawid', 25),\n",
       " ('Dawid', 24),\n",
       " ('Lukasz', 90),\n",
       " ('Lukasz', 91),\n",
       " ('Maciej', 87),\n",
       " ('Maciej', 82)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey('ascending').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lukasz [90, 91]\n",
      "Maciej [87, 82]\n",
      "Artur [64, 63]\n",
      "Dawid [25, 24]\n",
      "Bartosz [76, 77]\n"
     ]
    }
   ],
   "source": [
    "dict_rdd = rdd.groupByKey().collect()\n",
    "for key, value in dict_rdd:\n",
    "    print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lukasz 2\n",
      "Maciej 2\n",
      "Artur 2\n",
      "Dawid 2\n",
      "Bartosz 2\n"
     ]
    }
   ],
   "source": [
    "dict_rdd = rdd.countByKey().items()\n",
    "for key,value in dict_rdd:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark RDD `Grouping` and `binning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupby` returns a RDD of grouped elements (iterable) as per a given group operation (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 4, 6, 8, 10, 12, 14]), (1, [3, 5, 7, 9, 11, 13, 15])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "result = rdd.groupBy(lambda x:x%2).collect()\n",
    "sorted([(x, sorted(y)) for (x,y) in result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `histogram` method takes a list of bins/buckets and returns a tuple with the result of the histogram (binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x:x*x)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 10, 20, 30, 40, 50, 60, 70, 80, 90], [2, 1, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.histogram([x for x in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Set operations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create smaller RDDs to demonstrate joint operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddA: [5, 2, 1]\n",
      "rddB: [7, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "list1 = np.random.randint(0,10,3)\n",
    "rddA = spark.sparkContext.parallelize(list1)\n",
    "list2 = np.random.randint(0,10,3)\n",
    "rddB = spark.sparkContext.parallelize(list2)\n",
    "print(\"rddA:\",rddA.collect())\n",
    "print(\"rddB:\",rddB.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rddA` + `rddB` gives the union (like set union), not the element wise sum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 1, 7, 2, 4]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rddA + rddB).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cartesian` gives the pairwise product (as tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 7), (5, 2), (5, 4), (2, 7), (2, 2), (2, 4), (1, 7), (1, 2), (1, 4)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.cartesian(rddB).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `intersection` and `subtract` methods return a RDD of the set intersection and subtraction (difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 10, 3, 11, 4, 12, 5, 6, 7]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,10,11,12]\n",
    "data2 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "rddA = spark.sparkContext.parallelize(data)\n",
    "rddB = spark.sparkContext.parallelize(data2)\n",
    "\n",
    "rddA.intersection(rddB).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.subtract(rddB).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the `SparkContext` at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning the count of each value in RDD as a dictionary, then ordering operations and use items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "# sc = Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"D:\\My commercial projects\\PySpark\\Project 5 PySpark RDD\\data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6110\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21201\n"
     ]
    }
   ],
   "source": [
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue() \n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum values for each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RODExercises\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"D:\\My commercial projects\\PySpark\\Project 5 PySpark RDD/sensors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1,2016-01-01,20.5', 's2,2016-01-01,30.1', 's1,2016-01-02,60.2', 's2,2016-01-02,20.4', 's1,2016-01-03,55.5', 's2,2016-01-03,52.5']\n"
     ]
    }
   ],
   "source": [
    "print(lines.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s1', '2016-01-01', '20.5'], ['s2', '2016-01-01', '30.1'], ['s1', '2016-01-02', '60.2'], ['s2', '2016-01-02', '20.4'], ['s1', '2016-01-03', '55.5'], ['s2', '2016-01-03', '52.5']]\n"
     ]
    }
   ],
   "source": [
    "sensorsplit = lines.map(lambda x:x.split(','))\n",
    "print(sensorsplit.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s1', 20.5), ('s2', 30.1), ('s1', 60.2), ('s2', 20.4), ('s1', 55.5), ('s2', 52.5)]\n"
     ]
    }
   ],
   "source": [
    "sensorsplitpm10 = sensorsplit.map(lambda x:(x[0], float(x[2])))\n",
    "print(sensorsplitpm10.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensors with at least 2 readings with a PM10 value greater than the critical threshold 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensorandpm10split = lines.map(lambda x:x.split(',')).map(lambda x:(x[0], float(x[2]))).filter(lambda x:x[1] >50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s1', 60.2), ('s1', 55.5), ('s2', 52.5)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensorandpm10split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'s1': 2, 's2': 1})\n"
     ]
    }
   ],
   "source": [
    "print(sensorandpm10split.countByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s1', 2), ('s2', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sensorandpm10split.groupByKey().mapValues(len).sortBy(lambda x:x[1], False).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sensorID and the list of dates with a PM10 values greater than 50 for that sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1,2016-01-01,20.5', 's2,2016-01-01,30.1', 's1,2016-01-02,60.2', 's2,2016-01-02,20.4', 's1,2016-01-03,55.5', 's2,2016-01-03,52.5']\n"
     ]
    }
   ],
   "source": [
    "print(lines.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensorandpm10split = lines.map(lambda x:x.split(',')).filter(lambda x: float(x[2])>50.0).map(lambda x:(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s1', '2016-01-02'), ('s1', '2016-01-03'), ('s2', '2016-01-03')]\n"
     ]
    }
   ],
   "source": [
    "print(sensorandpm10split.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s1': ['2016-01-02', '2016-01-03'], 's2': ['2016-01-03']}\n"
     ]
    }
   ],
   "source": [
    "print(sensorandpm10split.groupByKey().mapValues(list).collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark using `RDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RDD Hamlet\")\n",
    "sc = SparkContext(conf = conf)\n",
    "poem_rdd1 = sc.textFile(\"D:\\My commercial projects\\PySpark\\Project 5 PySpark RDD/hamlet.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://POL-8625NG3.SIIPOLSKA.PL:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD Hamlet</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=RDD Hamlet>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D:\\My commercial projects\\PySpark\\Project 5 PySpark RDD/hamlet.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To be, or not to be, that is the question:',\n",
       " \"Whether 'tis nobler in the mind to suffer\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_rdd1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To be, or not to be, that is the question:', \"Whether 'tis nobler in the mind to suffer\", 'The slings and arrows of outrageous fortune,', 'Or to take arms against a sea of troubles', 'And by opposing end them. To die—to sleep,', 'No more; and by a sleep to say we end', 'The heart-ache and the thousand natural shocks', \"That flesh is heir to: 'tis a consummation\", \"Devoutly to be wish'd. To die, to sleep;\", \"To sleep, perchance to dream—ay, there's the rub:\", 'For in that sleep of death what dreams may come,', 'When we have shuffled off this mortal coil,', \"Must give us pause—there's the respect\", 'That makes calamity of so long life.', 'For who would bear the whips and scorns of time,', \"Th'oppressor's wrong, the proud man's contumely,\", \"The pangs of dispriz'd love, the law's delay,\", 'The insolence of office, and the spurns', \"That patient merit of th'unworthy takes,\", 'When he himself might his quietus make', 'With a bare bodkin? Who would fardels bear,', 'To grunt and sweat under a weary life,', 'But that the dread of something after death,', \"The undiscovere'd country, from whose bourn\", 'No traveller returns, puzzles the will,', 'And makes us rather bear those ills we have', 'Than fly to others that we know not of?', 'Thus conscience does make cowards of us all,', 'And thus the native hue of resolution', \"Is sicklied o'er with the pale cast of thought,\", 'And enterprises of great pitch and moment', 'With this regard their currents turn awry', 'And lose the name of action.']\n"
     ]
    }
   ],
   "source": [
    "print(poem_rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Map` and `Reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 41, 44, 41, 42, 37, 46, 42, 40, 49, 48, 43, 38, 36, 48, 48, 45, 39, 40, 38, 43, 38, 44, 43, 39, 43, 39, 44, 37, 47, 41, 41, 28]\n",
      "Total lines: 1374\n"
     ]
    }
   ],
   "source": [
    "lineLengths = poem_rdd1.map(lambda s: len(s)) # transformation\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b) # action\n",
    "print(lineLengths.collect())\n",
    "print('Total lines:', totalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the rdd item `count` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd = sc.parallelize([1, 2, 3, 4, 5],2)\n",
    "my_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(my_rdd.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show `the number of partitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of partitions in RDD:  2\n",
      "[[1, 2], [3, 4, 5]]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print('The number of partitions in RDD:  {0}'.format(my_rdd.getNumPartitions()))\n",
    "print(my_rdd.glom().collect()) \n",
    "print(my_rdd.collect()) # it shows single list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a defined function with `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['to', 'be,', 'or', 'not', 'to', 'be,', 'that', 'is', 'the', 'question:'],\n",
       " ['whether', \"'tis\", 'nobler', 'in', 'the', 'mind', 'to', 'suffer'],\n",
       " ['the', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune,'],\n",
       " ['or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles'],\n",
       " ['and', 'by', 'opposing', 'end', 'them.', 'to', 'die—to', 'sleep,']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transfunc(lines):\n",
    "    lines = lines.lower() # taking each line and converts it to lower case\n",
    "    lines = lines.split() # then, each line was splited\n",
    "    return lines # it returns each word separately by commas\n",
    "\n",
    "poem_rdd2 = poem_rdd1.map(transfunc)\n",
    "poem_rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `flatMap` to flatten out the rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'be,',\n",
       " 'or',\n",
       " 'not',\n",
       " 'to',\n",
       " 'be,',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'question:',\n",
       " 'whether',\n",
       " \"'tis\",\n",
       " 'nobler',\n",
       " 'in',\n",
       " 'the']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_rdd3 = poem_rdd1.flatMap(transfunc) #each word is a separate element\n",
    "poem_rdd3.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `distinct values` in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'be,', 'or', 'not', 'that']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_rdd3.distinct().take(5) # Transformation followed by Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `filter` to apply a search condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be,',\n",
       " 'or',\n",
       " 'not',\n",
       " 'be,',\n",
       " 'that',\n",
       " 'is',\n",
       " 'question:',\n",
       " 'whether',\n",
       " \"'tis\",\n",
       " 'nobler']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipwords = ['to', 'the', 'of']\n",
    "poem_rdd4 = poem_rdd3.filter(lambda x: x not in skipwords)\n",
    "poem_rdd4.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting `statistics` on an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 1, 124750, 20750.0, 144.04860290887933)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_rdd = sc.parallelize(range(1,500))\n",
    "numbers_rdd.max(), numbers_rdd.min(), numbers_rdd.sum(), numbers_rdd.variance(), numbers_rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
